{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TANZHIJIAN14/A-E-Chatbot/blob/main/A%26E_chatbot__fine_tuning_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ko9zoFeyrij"
      },
      "source": [
        "# Util function and variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "VpUVULJ-yq65"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "uncleaned_dataset_path = '/content/sample_data/dataset/'\n",
        "cleaned_dataset_path = '/content/sample_data/cleaned_dataset/'\n",
        "tokenized_dataset_path = '/content/sample_data/tokenized_dataset/'\n",
        "\n",
        "def replace_empty_value(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    # Replace empty strings and other missing value indicators with NaN\n",
        "    df.replace(\"\", np.nan, inplace=True)\n",
        "    return df\n",
        "\n",
        "def remove_columns(file_path, columns_to_remove):\n",
        "    print('Called')\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Remove the specified columns\n",
        "    for column in columns_to_remove:\n",
        "        if column in df.columns:\n",
        "            df = df.drop(columns=[column])\n",
        "\n",
        "    return df\n",
        "\n",
        "def rename_columns(file_path, column_mapping):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Iterate over each key-value pair in the column mapping dictionary\n",
        "    for old_name, new_name in column_mapping.items():\n",
        "        # Check if the old column name exists before renaming\n",
        "        if old_name in df.columns:\n",
        "            # Rename the column\n",
        "            df.rename(columns={old_name: new_name}, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "def concat_files(checkFile):\n",
        "    # List to hold DataFrames\n",
        "    dfs = []\n",
        "\n",
        "    # Loop through each CSV file in the directory\n",
        "    for filename in all_processed_files_path:\n",
        "        # Read the CSV file into a DataFrame and append it to the list\n",
        "        to_be_combine_file = filename.split('/')[-1][0:12]\n",
        "        if to_be_combine_file == checkFile:\n",
        "          df = pd.read_csv(filename)\n",
        "          dfs.append(df)\n",
        "\n",
        "          #Remove the file\n",
        "          if os.path.exists(filename):\n",
        "            os.remove(filename)\n",
        "\n",
        "    # Concatenate all DataFrames in the list along the rows axis (axis=0)\n",
        "    if len(dfs) > 0:\n",
        "      return pd.concat(dfs, axis=0, ignore_index=True)\n",
        "    else:\n",
        "      return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RebCd2O-zdyK"
      },
      "source": [
        "# Load file paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "E951DC9UzgDs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "all_files_path = glob.glob(os.path.join(uncleaned_dataset_path, \"*.csv\"))\n",
        "all_processed_files_path = glob.glob(os.path.join(cleaned_dataset_path, \"*.csv\"))\n",
        "\n",
        "#'/content/dataset/guideLine.csv'\n",
        "#split -> 'guideLine.csv'\n",
        "all_files_name = [file.split('/')[-1] for file in all_files_path]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYhxKgXwa_Xi"
      },
      "source": [
        "# Replace the empty value with NaN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "a2ML3K2xbCmf"
      },
      "outputs": [],
      "source": [
        "# List to store cleaned DataFrames\n",
        "cleaned_dfs = []\n",
        "\n",
        "for file_path in all_files_path:\n",
        "    cleaned_df = replace_empty_value(file_path)\n",
        "    cleaned_dfs.append(cleaned_df)\n",
        "\n",
        "# Save each cleaned DataFrame individually and overwrite the original dataset\n",
        "for i, df in enumerate(cleaned_dfs):\n",
        "    df.to_csv(cleaned_dataset_path + all_files_name[i], index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI1Xgt9ass_P"
      },
      "source": [
        "# Remove columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEIUJdYesmcU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Name of column to be removed\n",
        "columns_to_remove = ['web-scraper-order', 'web-scraper-start-url']\n",
        "\n",
        "# List to store cleaned DataFrames\n",
        "cleaned_dfs = []\n",
        "\n",
        "for file_path in all_processed_files_path:\n",
        "    cleaned_df = remove_columns(file_path, columns_to_remove)\n",
        "    cleaned_dfs.append(cleaned_df)\n",
        "\n",
        "\n",
        "# Save each cleaned DataFrame individually and overwrite the original dataset\n",
        "for i, df in enumerate(cleaned_dfs):\n",
        "    df.to_csv(cleaned_dataset_path + all_files_name[i], index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9hvCMM875xU"
      },
      "source": [
        "# Rename column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "vCK-BlMp78lz"
      },
      "outputs": [],
      "source": [
        "# Define a dictionary of old column names mapped to new column names\n",
        "column_mapping = {\n",
        "    'link': 'linkName',\n",
        "    'link-href': 'link',\n",
        "    'paymentLink': 'paymentName',\n",
        "    'paymentLink-href': 'paymentLink',\n",
        "    'eventLink': 'eventName',\n",
        "    'eventLink-href': 'eventLink',}\n",
        "\n",
        "# List to store cleaned DataFrames\n",
        "cleaned_dfs = []\n",
        "for file_path in all_processed_files_path:\n",
        "    cleaned_df = rename_columns(file_path, column_mapping)\n",
        "    cleaned_dfs.append(cleaned_df)\n",
        "\n",
        "# Save each cleaned DataFrame individually and overwrite the original dataset\n",
        "for i, df in enumerate(cleaned_dfs):\n",
        "    df.to_csv(cleaned_dataset_path + all_files_name[i], index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwThrqJK94fk"
      },
      "source": [
        "# Combine and remove csv files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "0yBUq-sL_53d"
      },
      "outputs": [],
      "source": [
        "combined_df = concat_files('visitingHour')\n",
        "\n",
        "if combined_df is not None:\n",
        "  # Save the combined DataFrame to a new CSV file\n",
        "  combined_file_path = cleaned_dataset_path + 'allVisitingHour.csv'\n",
        "  combined_df.to_csv(combined_file_path, index=False)\n",
        "\n",
        "# Reset the all cleaned files path\n",
        "all_processed_files_path = glob.glob(os.path.join(cleaned_dataset_path, \"*.csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcj_7aOesOyO"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_rIXRQlsUev"
      },
      "outputs": [],
      "source": [
        "# Need to run this once per machine\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiIMXRRxoH0T"
      },
      "source": [
        "# Load Tokenizer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "89PLrigUo5U4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        ")\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "#Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id =  tokenizer.unk_token_id\n",
        "tokenizer.padding_side = 'left'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtR0cs9fxjR9"
      },
      "source": [
        "# Formatting prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "TleVwfXDxlIk"
      },
      "outputs": [],
      "source": [
        "def formatting_func(example):\n",
        "    text = f\"### The following is a doctor's opinion on a person's query: \\n### Patient query: {example['input']} \\n### Doctor opinion: {example['output']}\"\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bACpp1QDxLdn"
      },
      "source": [
        "# Tokenization dataset setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "PbZE5DAYxNgk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "max_length = 512 # differs from datasets to datasets\n",
        "\n",
        "def tokenize_dataset(file):\n",
        "  # Load the CSV file into a pandas DataFrame\n",
        "  df = pd.read_csv(file)\n",
        "\n",
        "  # Function to tokenize a single text column\n",
        "  def tokenize_column(series, tokenizer):\n",
        "      return series.apply(lambda x: tokenizer(str(x), padding='max_length', truncation=True, return_tensors='pt'))\n",
        "\n",
        "  # Apply the tokenizer to all columns\n",
        "  tokenized_data = {col: tokenize_column(df[col], tokenizer) for col in df.columns}\n",
        "\n",
        "  # Combine tokenized columns into the DataFrame\n",
        "  for col in df.columns:\n",
        "      df[col + '_input_ids'] = tokenized_data[col].apply(lambda x: x['input_ids'].squeeze().tolist())\n",
        "      df[col + '_attention_mask'] = tokenized_data[col].apply(lambda x: x['attention_mask'].squeeze().tolist())\n",
        "\n",
        "  # Optionally, drop the original columns if only tokenized data is needed\n",
        "  df.drop(columns=[col for col in df.columns if not col.endswith('_input_ids') and not col.endswith('_attention_mask')], inplace=True)\n",
        "\n",
        "  # Define the columns to keep in the split datasets\n",
        "  tokenized_columns = [col for col in df.columns if col.endswith('_input_ids') or col.endswith('_attention_mask')]\n",
        "\n",
        "  # Split the dataset\n",
        "  train_df, eval_df = train_test_split(df[tokenized_columns], test_size=0.2, random_state=42)\n",
        "\n",
        "  # Optionally reset the index\n",
        "  train_df.reset_index(drop=True, inplace=True)\n",
        "  eval_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  # Save the training and evaluation DataFrames to CSV files\n",
        "  train_df.to_csv(tokenized_dataset_path + 'train_' + file.split('/')[-1], index=False)\n",
        "  eval_df.to_csv(tokenized_dataset_path + 'eval_'+ file.split('/')[-1], index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing dataset"
      ],
      "metadata": {
        "id": "gFYDQiPv2lup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load all the dataset\n",
        "for file_path in all_processed_files_path:\n",
        "    df = pd.read_csv(file_path)\n",
        "    if len(df) > 1:\n",
        "        tokenize_dataset(file_path)"
      ],
      "metadata": {
        "id": "3NhgO2LB2oaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWxc_uOZ3euf"
      },
      "source": [
        "# Load base LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMsGrV4K3gjP"
      },
      "outputs": [],
      "source": [
        "compute_dtype = getattr(torch, \"float16\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "          model_name, quantization_config=bnb_config, device_map={\"\": 0}\n",
        ")\n",
        "#Configure the pad token in the model\n",
        "model.config.pad_token_id = tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize streaming class"
      ],
      "metadata": {
        "id": "3OR_4Fhelzdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StreamCallback:\n",
        "    def __init__(self, tokenizer, line_length=80):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.current_word = \"\"\n",
        "        self.line_length = line_length\n",
        "        self.current_length = 0\n",
        "\n",
        "    def process_token(self, token_id):\n",
        "        # Decode the token id to a string\n",
        "        token = self.tokenizer.decode(token_id, skip_special_tokens=True)\n",
        "\n",
        "        if token.strip() == \"\":\n",
        "            # This is a space\n",
        "            if self.current_word:\n",
        "                if self.current_length + len(self.current_word) > self.line_length:\n",
        "                    # If adding the current word exceeds line length, print a new line first\n",
        "                    print()\n",
        "                    self.current_length = 0\n",
        "                print(self.current_word, end='', flush=True)\n",
        "                self.current_length += len(self.current_word)\n",
        "                self.current_word = \"\"\n",
        "            # Print the space\n",
        "            print(token, end='', flush=True)\n",
        "            self.current_length += len(token)\n",
        "        else:\n",
        "            # This is a part of a word or punctuation\n",
        "            if self.current_word and token in {\".\", \",\", \"!\", \"?\", \";\", \":\"}:\n",
        "                if self.current_length + len(self.current_word) > self.line_length:\n",
        "                    # If adding the current word exceeds line length, print a new line first\n",
        "                    print()\n",
        "                    self.current_length = 0\n",
        "                # Print the current word followed by punctuation without a space\n",
        "                print(self.current_word, end='', flush=True)\n",
        "                print(token, end='', flush=True)\n",
        "                self.current_length += len(self.current_word) + len(token)\n",
        "                self.current_word = \"\"\n",
        "            else:\n",
        "                # Accumulate characters into a word\n",
        "                self.current_word += token\n",
        "\n",
        "    def finalize(self):\n",
        "        # Print any remaining word\n",
        "        if self.current_word:\n",
        "            if self.current_length + len(self.current_word) > self.line_length:\n",
        "                # If adding the current word exceeds line length, print a new line first\n",
        "                print()\n",
        "            print(self.current_word, end='', flush=True)"
      ],
      "metadata": {
        "id": "t3hIYFYxl2g2"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define prompt for loaded LLM"
      ],
      "metadata": {
        "id": "537gaiOnu6Z0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "NYwiXY8U5vLU"
      },
      "outputs": [],
      "source": [
        "from transformers import GenerationConfig\n",
        "\n",
        "def generate_with_streaming(instruction, model, tokenizer):\n",
        "    prompt = \"[INST] \" + instruction + \" [/INST]\\n\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
        "    callback = StreamCallback(tokenizer)\n",
        "\n",
        "    # Define generation config\n",
        "    generation_config = GenerationConfig(\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        temperature=1.0,\n",
        "        top_p=1.0,\n",
        "        top_k=50,\n",
        "        num_beams=1,\n",
        "        max_new_tokens=256\n",
        "    )\n",
        "\n",
        "    # Generate tokens one by one\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "    generated_ids = input_ids\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(generation_config.max_new_tokens):\n",
        "            outputs = model(\n",
        "                input_ids=generated_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                return_dict=True\n",
        "            )\n",
        "            next_token_logits = outputs.logits[:, -1, :]\n",
        "            next_token_id = torch.argmax(next_token_logits, dim=-1)\n",
        "            next_token_id = next_token_id.unsqueeze(-1)\n",
        "\n",
        "            # Process the token\n",
        "            callback.process_token(next_token_id.item())\n",
        "\n",
        "            # Append the token to the generated_ids\n",
        "            generated_ids = torch.cat((generated_ids, next_token_id), dim=1)\n",
        "            attention_mask = torch.cat((attention_mask, torch.ones_like(next_token_id)), dim=1)\n",
        "\n",
        "            # Break if end-of-sequence token is generated\n",
        "            if next_token_id.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # Finalize the callback to print any remaining word\n",
        "    callback.finalize()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Demo"
      ],
      "metadata": {
        "id": "FaIFtUd8jUSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "instruction = \"Tell me what is large language model\"\n",
        "generate_with_streaming(instruction, model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzyQIUtHjV8p",
        "outputId": "e08a69b6-6d53-44cb-e61c-2cbb9a746383"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Alargelanguagemodelisatypeofartificialintelligence(AI)modelthatisdesignedtoprocessandgeneratehumanlanguage.\n",
            "Itistypicallyatypeofdeeplearningmodelthatistrainedonalargeamountoftextdata,\n",
            "suchasbooks,articles,andconversations.\n",
            "Largelanguagemodelsarecapableofunderstandingthecontextandmeaningofwordsandphrases,\n",
            "andcanbeusedforavarietyofnaturallanguageprocessingtasks,\n",
            "suchaslanguagetranslation,sentimentanalysis,andtextsummarization.\n",
            "Theyarealsocapableofgeneratinghuman-liketext,\n",
            "andcanbeusedfortaskssuchaschatbotsandlanguagegeneration."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1H19-qxO-V8bFG7OwW8hvgRqCATX3WRXQ",
      "authorship_tag": "ABX9TyP1namj8EQhYtVEYoRqh8bw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}