{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1H19-qxO-V8bFG7OwW8hvgRqCATX3WRXQ",
      "authorship_tag": "ABX9TyMxDdw/YHKm6cvRmBwGXRSQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TANZHIJIAN14/A-E-Chatbot/blob/main/A%26E_chatbot_data_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google drive"
      ],
      "metadata": {
        "id": "gKGpTqDhN7EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vazTWw37N-JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Util function and variable"
      ],
      "metadata": {
        "id": "-Ko9zoFeyrij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "uncleaned_dataset_path = '/content/drive/MyDrive/YR3_SEM2/FYP_I/dataset/'\n",
        "cleaned_dataset_path = '/content/drive/MyDrive/YR3_SEM2/FYP_I/cleaned_dataset/'\n",
        "\n",
        "def remove_columns(file_path, columns_to_remove):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Remove the specified columns\n",
        "    df.drop(columns=columns_to_remove, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "def rename_columns(file_path, column_mapping):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Iterate over each key-value pair in the column mapping dictionary\n",
        "    for old_name, new_name in column_mapping.items():\n",
        "        # Check if the old column name exists before renaming\n",
        "        if old_name in df.columns:\n",
        "            # Rename the column\n",
        "            df.rename(columns={old_name: new_name}, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "def concat_files(checkFile):\n",
        "    # List to hold DataFrames\n",
        "    dfs = []\n",
        "\n",
        "    # Loop through each CSV file in the directory\n",
        "    for filename in all_processed_files_path:\n",
        "        # Read the CSV file into a DataFrame and append it to the list\n",
        "        to_be_combine_file = filename.split('/')[3][0:12]\n",
        "        if to_be_combine_file == checkFile:\n",
        "          df = pd.read_csv(filename)\n",
        "          dfs.append(df)\n",
        "\n",
        "    # Concatenate all DataFrames in the list along the rows axis (axis=0)\n",
        "    return pd.concat(dfs, axis=0, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "VpUVULJ-yq65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load file paths"
      ],
      "metadata": {
        "id": "RebCd2O-zdyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "all_files_path = glob.glob(os.path.join(uncleaned_dataset_path, \"*.csv\"))\n",
        "all_processed_files_path = glob.glob(os.path.join(cleaned_dataset_path, \"*.csv\"))\n",
        "\n",
        "#'/content/dataset/guideLine.csv'\n",
        "#split -> 'guideLine.csv'\n",
        "all_files_name = [file.split('/')[3] for file in all_files_path]"
      ],
      "metadata": {
        "id": "E951DC9UzgDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove columns"
      ],
      "metadata": {
        "id": "KI1Xgt9ass_P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEIUJdYesmcU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Name of column to be removed\n",
        "columns_to_remove = ['web-scraper-order', 'web-scraper-start-url']\n",
        "\n",
        "# List to store cleaned DataFrames\n",
        "cleaned_dfs = []\n",
        "\n",
        "for file_path in all_files_path:\n",
        "    cleaned_df = remove_columns(file_path, columns_to_remove)\n",
        "    cleaned_dfs.append(cleaned_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the cleaned files"
      ],
      "metadata": {
        "id": "BSxFYJoc10Le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save each cleaned DataFrame individually and overwrite the original dataset\n",
        "for i, df in enumerate(cleaned_dfs):\n",
        "    df.to_csv(cleaned_dataset_path + all_files_name[i], index=False)"
      ],
      "metadata": {
        "id": "9lSMzEv614G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rename column"
      ],
      "metadata": {
        "id": "_9hvCMM875xU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary of old column names mapped to new column names\n",
        "column_mapping = {\n",
        "    'link': 'linkName',\n",
        "    'link-href': 'link',\n",
        "    'paymentLink': 'paymentName',\n",
        "    'paymentLink-href': 'paymentLink'}\n",
        "\n",
        "# List to store cleaned DataFrames\n",
        "cleaned_dfs = []\n",
        "\n",
        "for file_path in all_processed_files_path:\n",
        "    cleaned_df = rename_columns(file_path, column_mapping)\n",
        "    cleaned_dfs.append(cleaned_df)"
      ],
      "metadata": {
        "id": "vCK-BlMp78lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the renamed column dataset"
      ],
      "metadata": {
        "id": "MZ6WqqZK9BU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save each cleaned DataFrame individually and overwrite the original dataset\n",
        "for i, df in enumerate(cleaned_dfs):\n",
        "    df.to_csv(cleaned_dataset_path + all_files_name[i], index=False)"
      ],
      "metadata": {
        "id": "SJGWpyiy9FLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine csv files"
      ],
      "metadata": {
        "id": "zwThrqJK94fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = concat_files('visitingHour')\n",
        "\n",
        "# Save the combined DataFrame to a new CSV file\n",
        "combined_file_path = cleaned_dataset_path + 'allVisitingHour.csv'\n",
        "combined_df.to_csv(combined_file_path, index=False)"
      ],
      "metadata": {
        "id": "0yBUq-sL_53d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}